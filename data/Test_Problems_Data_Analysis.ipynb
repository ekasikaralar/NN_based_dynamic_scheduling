{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5a31ed8",
   "metadata": {},
   "source": [
    "# Test Problems Data Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0067b035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from typing import Union, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce85d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 42 # we fix a random seed for reproducibility \n",
    "\n",
    "precision = 5  # Precision of 5 minutes\n",
    "\n",
    "# we focus on the time period between 7 am and midnight\n",
    "start_hour = 7 \n",
    "end_hour = 24\n",
    "\n",
    "# time period according to 5 minutes\n",
    "time_focus_start = start_hour * 60 // precision  # 7 AM\n",
    "time_focus_end = end_hour * 60 // precision   # Midnight\n",
    "\n",
    "# total length of the time horizon in 5 minutes\n",
    "length_time = int(time_focus_end - time_focus_start)\n",
    "\n",
    "# time constants\n",
    "MINUTES_IN_HOUR = 60\n",
    "SECONDS_IN_HOUR = 3600\n",
    "\n",
    "# scaling constant\n",
    "SCALING_FACTOR = 400\n",
    "\n",
    "#path to the folder including the csv files\n",
    "data_folder_path = 'MayJulyWeekdays/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bcb584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_list(folder_path, file_pattern):\n",
    "    \"\"\"\n",
    "    Generates a list of file paths from a specified folder that match a given pattern.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path: str - Path to the folder where files are located.\n",
    "    - file_pattern: str - Pattern to match files.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of file paths matching the pattern in the specified folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(folder_path):\n",
    "        raise FileNotFoundError(f\"The specified folder path does not exist: {folder_path}\")\n",
    "\n",
    "    file_list = glob.glob(os.path.join(folder_path, file_pattern))\n",
    "\n",
    "    if not file_list:\n",
    "        print(f\"No files found in {folder_path} matching the pattern '{file_pattern}'\")\n",
    "\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68b0953",
   "metadata": {},
   "source": [
    "## 1. Main Test Problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2708b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_no = 17 #main test problem has 17 dimensions (17 different classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53262e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a folder for saving the system parameters\n",
    "\n",
    "main_test_folder_path = os.path.join(data_folder_path,f\"problem_{class_no}dim\")\n",
    "os.makedirs(main_test_folder_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b603fc",
   "metadata": {},
   "source": [
    "### 1.1 Pre-limit arrival process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1604535",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list_cust_subcalls = get_file_list(data_folder_path, file_pattern=\"*_cust_subcalls.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3a51a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filt_arrivals(file_path,precision,service_lookup,service,node=0):\n",
    "    \"\"\"\n",
    "    Filter and process call arrival data.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str - Path to the data file.\n",
    "    - precision: int - Time precision for resampling.\n",
    "    - service_lookup: bool - Whether to filter by service.\n",
    "    - service: int - Service type to filter.\n",
    "    - node: int - Node number to filter, default is 0.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Processed data with columns ['index', 'call_id'].\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "    except Exception as e:\n",
    "        raise FileNotFoundError(f\"Error reading file {file_path}: {e}\")\n",
    "        \n",
    "    \n",
    "    \"\"\"Remove outliers based on queue and service time thresholds.\"\"\"\n",
    "    data = data[data[\"queue_time\"] < 900] \n",
    "    data = data[data[\"service_time\"] < 1800] \n",
    "    \n",
    "    \"\"\"Remove calls with specified abnormal outcomes.\"\"\"\n",
    "    abnormal_outcomes = [4,13,14,23,30,40,50]\n",
    "    calls_with_abnormal_outcome = data[data[\"outcome\"].isin(abnormal_outcomes)]\n",
    "    call_ids_abnormal_outcome = calls_with_abnormal_outcome[\"call_id\"].unique()\n",
    "    drop_index = data[data[\"call_id\"].isin(call_ids_abnormal_outcome)].index\n",
    "    data = data.drop(index = drop_index)\n",
    "    \n",
    "    #focusing only on the 1st customer subcalls\n",
    "    data = data[data[\"cust_subcall\"] == 1]\n",
    "    \n",
    "    #removing the multiple records of the same call\n",
    "    data = data.sort_values([\"segment_start\"])\n",
    "    data = data.drop_duplicates(\"call_id\",keep = \"last\")\n",
    "      \n",
    "    \"\"\"Filter data based on service and node criteria.\"\"\"\n",
    "    if service_lookup == True:\n",
    "        data = data[data[\"service\"] == service]\n",
    "        \n",
    "        if (service == 1) and (node != 0):\n",
    "            data = data[data[\"node\"].isin([node])]\n",
    "        elif (service == 1) and (node == 0):\n",
    "            node_drop_index = data[(data[\"service\"] == 1) & data[\"node\"].isin([5,6,7])].index #keep only nodes 1,2,3\n",
    "            data = data.drop(index = node_drop_index)\n",
    "            \n",
    "    \n",
    "    #some calls when they first arrive are on hold and then are transferred to the agents but since they have the same call id we look at their first occurrence\n",
    "    data = data.reset_index()\n",
    "    \n",
    "    #defining the start time of the events\n",
    "    start = []\n",
    "    for i in range(len(data)):\n",
    "        start.append(datetime.fromtimestamp(data[\"segment_start\"][i]+5*60*60).strftime(\"%A, %B %d, %Y %H:%M:%S\")) \n",
    "    \n",
    "    data[\"start\"] = start\n",
    "    data['Datetime'] = pd.to_datetime(data['start'])\n",
    "    data[\"day\"] = pd.to_datetime(start).date\n",
    "    new_data = data\n",
    "    drop_ind = new_data.drop_duplicates(subset = [\"day\"])\n",
    "    \n",
    "    if len(drop_ind) > 1:\n",
    "        drop_day = np.array(drop_ind.day)[1]\n",
    "        data = data[data[\"day\"] != drop_day]\n",
    "    \n",
    "    data = data.set_index('Datetime')\n",
    "    \n",
    "    #resolution\n",
    "    resolution = \"{pre}T\"\n",
    "    \n",
    "    #how many arrivals happened within the specific time interval\n",
    "    data = data.resample(resolution.format(pre = precision)).count()\n",
    "    data = data.dropna(axis = 0)\n",
    "    \n",
    "    #setting up a new column to show the time interval\n",
    "    new_index = []\n",
    "    for j in data.index:\n",
    "        h=j.hour\n",
    "        m=j.minute\n",
    "        item=str(h)+':'+str(m)\n",
    "        new_index.append(item)\n",
    "        \n",
    "    data[\"index\"] = new_index\n",
    "    data = data.reset_index()\n",
    "    \n",
    "    return data[[\"index\",\"call_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42526198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(hour, minute):\n",
    "    \"\"\"Formats time as 'hour:minute'.\"\"\"\n",
    "    return f\"{hour}:{minute:0d}\"\n",
    "    \n",
    "def create_time_intervals():\n",
    "    \"\"\"Generates a list of time intervals from '0:0' to '23:55' in 5-minute increments.\"\"\"\n",
    "    return [format_time(hour, minute) for hour in range(24) for minute in range(0, 60, 5)]\n",
    "\n",
    "def merge_daily_data(file_list, precision, service_lookup, service, node):\n",
    "    \"\"\"\n",
    "    Merges and processes data from multiple days.\n",
    "\n",
    "    Parameters:\n",
    "    - file_list: list of str - List of file paths.\n",
    "    - precision: int - Time precision for resampling.\n",
    "    - service_lookup: bool - Whether to filter by service.\n",
    "    - service: int - Service type to filter.\n",
    "    - node: int - Node number to filter.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Merged and processed data.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not file_list:\n",
    "        raise ValueError(\"The file list is empty.\")\n",
    "        \n",
    "    time_intervals = create_time_intervals()\n",
    "    main_dataframe = pd.DataFrame(index=pd.Index(time_intervals))\n",
    "    main_dataframe = main_dataframe.merge(filt_arrivals(file_list[0],precision,service_lookup,service,node), how = \"left\", left_on = main_dataframe.index, right_on = \"index\", suffixes=('', '_0'))\n",
    "    main_dataframe = main_dataframe.fillna(0)\n",
    "    \n",
    "    for i in range(1,len(file_list)):\n",
    "        \n",
    "        #merging all the days based on their time arrival \n",
    "        main_dataframe = main_dataframe.merge(filt_arrivals(file_list[i],precision,service_lookup,service,node), how = \"left\", left_on = \"index\", right_on = \"index\", suffixes=('', f'_{i}'))\n",
    "    \n",
    "    #dropping the null values\n",
    "    main_dataframe = main_dataframe.fillna(0)\n",
    "    \n",
    "    #setting the index to show the arrival intervals\n",
    "    main_dataframe = main_dataframe.set_index(\"index\")\n",
    "    \n",
    "    \n",
    "    return main_dataframe\n",
    "        \n",
    "\n",
    "def means_totals(file_list, precision, service_lookup, service, node=0):\n",
    "    \"\"\"\n",
    "    Calculates the average arrival rate over multiple days.\n",
    "\n",
    "    Parameters:\n",
    "    - file_list: list of str - List of file paths.\n",
    "    - precision: int - Time precision for resampling.\n",
    "    - service_lookup: bool - Whether to filter by service.\n",
    "    - service: int - Service type to filter.\n",
    "    - node: int - Node number to filter, default is 0.\n",
    "\n",
    "    Returns:\n",
    "    - Series: Average arrival rate for each time interval.\n",
    "    \"\"\"\n",
    "    merged_data = merge_daily_data(file_list, precision, service_lookup, service, node)\n",
    "    return merged_data.mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b3353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_class_data(class_index, file_list, precision, nodes=None):\n",
    "    \"\"\"\n",
    "    Processes data for a given class index and nodes.\n",
    "\n",
    "    Parameters:\n",
    "    - class_index: int - The class index to process.\n",
    "    - file_list: list - List of file paths for data processing.\n",
    "    - precision: int - Time precision for resampling.\n",
    "    - nodes: list or None - List of node numbers to process for the class. If None, process without nodes.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary of the processed data.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    if nodes:\n",
    "        for node in nodes:\n",
    "            key = f'class_{class_index}_node_{node}'\n",
    "            results[key] = means_totals(file_list, precision, service_lookup=True, service=class_index, node=node)\n",
    "    else:\n",
    "        key = f'class_{class_index}'\n",
    "        results[key] = means_totals(file_list, precision, service_lookup=True, service=class_index, node=0)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94da2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_csv(data, filename, time_slice=None):\n",
    "    \"\"\"\n",
    "    Saves given data to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame or Series - The data to be saved.\n",
    "    - filename: str - The name of the file to save the data.\n",
    "    - time_slice: tuple of int, optional - Start and end indices for slicing the data by time. If None, save all data.\n",
    "    \"\"\"\n",
    "    if time_slice:\n",
    "        sliced_data = data[time_slice[0]:time_slice[1]]\n",
    "    else:\n",
    "        sliced_data = data\n",
    "\n",
    "    try:\n",
    "        np.savetxt(filename, sliced_data, delimiter=\",\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data to {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6395358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_arrivals = means_totals(file_list_cust_subcalls, precision=5, service_lookup=False, service=0, node=0)\n",
    "file_name = os.path.join(main_test_folder_path, f\"main_test_total_arrivals_partial_{precision}min.csv\")\n",
    "save_data_to_csv(total_arrivals, file_name, time_slice=(time_focus_start, time_focus_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137bcc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the indices as they appear in the US Bank data set\n",
    "class_indices = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 17]\n",
    "\n",
    "# Results are stored in results_by_class\n",
    "results_by_class = {}\n",
    "\n",
    "for class_ind in class_indices:\n",
    "    if class_ind == 1:  # class 1 is the retail class\n",
    "        results_by_class.update(process_class_data(class_ind, file_list_cust_subcalls, precision, nodes=[1, 2, 3]))\n",
    "    else:\n",
    "        results_by_class.update(process_class_data(class_ind, file_list_cust_subcalls, precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931e847b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_ind in class_indices:\n",
    "    if class_ind == 1:  # Assuming class 1 is the retail class\n",
    "        # Retail class is divided into three nodes \n",
    "        for node_num in [1, 2, 3]:\n",
    "            file_name_all = os.path.join(main_test_folder_path, f\"main_test_arrivals_class{class_ind}_node{node_num}_all_{precision}min.csv\")\n",
    "            file_name_partial = os.path.join(main_test_folder_path, f\"main_test_arrivals_class{class_ind}_node{node_num}_partial_{precision}min.csv\")\n",
    "            save_data_to_csv(results_by_class[f\"class_{class_ind}_node_{node_num}\"], file_name_all)\n",
    "            save_data_to_csv(results_by_class[f\"class_{class_ind}_node_{node_num}\"], file_name_partial, time_slice=(time_focus_start, time_focus_end))\n",
    "    # remaining classes other than Retail class\n",
    "    else:\n",
    "        file_name_all = os.path.join(main_test_folder_path, f\"main_test_arrivals_class{class_ind}_all_{precision}min.csv\")\n",
    "        file_name_partial = os.path.join(main_test_folder_path, f\"main_test_arrivals_class{class_ind}_partial_{precision}min.csv\")\n",
    "        save_data_to_csv(results_by_class[f\"class_{class_ind}\"], file_name_all)\n",
    "        save_data_to_csv(results_by_class[f\"class_{class_ind}\"], file_name_partial, time_slice=(time_focus_start, time_focus_end))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8714201a",
   "metadata": {},
   "source": [
    "### 1.2 Service Times/Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1b6718",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list_agent_records = get_file_list(data_folder_path, file_pattern=\"*_agent_records.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33f1c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filt_service(file_path,service,node=0):\n",
    "    \"\"\"\n",
    "    Filter data based on service and node, removing outliers and abnormal outcomes.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str - Path to the data file.\n",
    "    - service: int - Service type to filter.\n",
    "    - node: int - Node number to filter, default is 0.\n",
    "\n",
    "    Returns:\n",
    "    - Series: Work time data after filtering.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "    except Exception as e:\n",
    "        raise FileNotFoundError(f\"Error reading file {file_path}: {e}\")\n",
    "        \n",
    "    data = data[data[\"queue_time\"] < 900] #eliminating outliers through distribution plot of queueing time\n",
    "    data = data[data[\"work_time\"] < 1800] #eliminating outliers through distribution plot of work time\n",
    "    \n",
    "    #eliminating abnormal outcomes\n",
    "    abnormal_outcomes = [4,13,14,23,30,40,50]\n",
    "    calls_with_abnormal_outcome = data[data[\"outcome\"].isin(abnormal_outcomes)]\n",
    "    call_ids_abnormal_outcome = calls_with_abnormal_outcome[\"call_id\"].unique()\n",
    "    drop_index = data[data[\"call_id\"].isin(call_ids_abnormal_outcome)].index\n",
    "    data = data.drop(index = drop_index)\n",
    "        \n",
    "    #focusing only on the 1st customer subcalls\n",
    "    data = data[data[\"cust_subcall\"] == 1]\n",
    "    data = data[data[\"service\"] == service]\n",
    "    \n",
    "    #if service is 1, we focus on splitting to different nodes for Retail class\n",
    "    if service == 1:\n",
    "        data = data[data[\"node\"] == node]\n",
    "        \n",
    "    return data[\"work_time\"] #in the database worktime is calculated in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cefec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def means_service(file_list, service, node):\n",
    "    \"\"\"\n",
    "    Calculates the average service time across multiple days for a given service and node.\n",
    "\n",
    "    Parameters:\n",
    "    - file_list: list of str - List of file paths.\n",
    "    - service: int - Service type to filter.\n",
    "    - node: int - Node number to filter.\n",
    "\n",
    "    Returns:\n",
    "    - float: Average service time.\n",
    "    \"\"\"\n",
    "    if not file_list:\n",
    "        raise ValueError(\"The file list is empty.\")\n",
    "\n",
    "    main_dataframe = pd.DataFrame(filt_service(file_list[0],service))\n",
    "    \n",
    "    for i in range(1,len(file_list)):\n",
    "        data = filt_service(file_list[i],service,node)\n",
    "        df = pd.DataFrame(data)\n",
    "        main_dataframe = pd.concat([main_dataframe,df],axis=0)\n",
    "        \n",
    "    if main_dataframe.empty:\n",
    "        return 0  # Return 0 or appropriate value if no data is available\n",
    "\n",
    "    return main_dataframe.mean()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e68f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_service_times(file_list, class_indices, class_names):\n",
    "    \"\"\"\n",
    "    Calculates service times (in seconds) for different classes and nodes.\n",
    "\n",
    "    Parameters:\n",
    "    - file_list: list of str - List of file paths.\n",
    "    - class_indices: list of int - List of class indices corresponding to each class name.\n",
    "    - class_names: list of str - List of class names.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Service times (in seconds) for each class.\n",
    "    \"\"\"\n",
    "    service_times_df = pd.DataFrame(columns=[\"Total\"], index=class_names)\n",
    "\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        node = i + 1 if i < 3 else 0  # First three are retail classes with specific nodes\n",
    "        try:\n",
    "            service_time = means_service(file_list, class_indices[i], node)\n",
    "            service_times_df.loc[class_name, \"Total\"] = service_time\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating service time for {class_name}: {e}\")\n",
    "            service_times_df.loc[class_name, \"Total\"] = None\n",
    "\n",
    "    return service_times_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb16d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"Retail_Node1\", \"Retail_Node2\", \"Retail_Node3\", \"Premier\", \"Business\", \"Platinum\", \"Consumer_Loans\", \"Online_Banking\", \"EBO\", \"Telesales\", \"Subanco\", \"Case_Quality\", \"Priority_Service\", \"AST\", \"CCO\", \"Brokerage\", \"BPS\"]\n",
    "\n",
    "service_class_indices = [1, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 17]\n",
    "\n",
    "service_times_17dim = calculate_service_times(file_list_agent_records, service_class_indices, class_names) #service times in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4e0008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the service times and hourly service rates\n",
    "file_name_service_times = os.path.join(main_test_folder_path, f\"service_times_{class_no}dim.csv\")\n",
    "np.savetxt(file_name_service_times, service_times_17dim, delimiter = \",\") #in seconds\n",
    "\n",
    "# hourly service rates\n",
    "file_name_hourly_mu = os.path.join(main_test_folder_path, f\"mu_hourly_{class_no}dim.csv\")\n",
    "np.savetxt(file_name_hourly_mu, SECONDS_IN_HOUR/service_times_17dim, delimiter = \",\", fmt=\"%.2f\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6031a0fc",
   "metadata": {},
   "source": [
    "### 1.3 Abandonment Times/Rates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bee0b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filt_abandon(file_path,service,node=0):\n",
    "    \"\"\"\n",
    "    Filters abandonment times from data for a specific service and node.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str - Path to the data file.\n",
    "    - service: int - Service type to filter.\n",
    "    - node: int - Node number to filter, default is 0.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Filtered data with abandonment times.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "    except Exception as e:\n",
    "        raise FileNotFoundError(f\"Error reading file {file_path}: {e}\")\n",
    "    \n",
    "    # Eliminating outliers through distribution plot of queueing and service times\n",
    "    queue_time_threshold = 1100 if service == 10 else 900\n",
    "    data = data[(data[\"queue_time\"] < queue_time_threshold) & (data[\"service_time\"] < 1800)]\n",
    "\n",
    "    # Removing calls with abnormal outcomes\n",
    "    abnormal_outcomes = [4,13,14,23,30,40,50]\n",
    "    calls_with_abnormal_outcome = data[data[\"outcome\"].isin(abnormal_outcomes)]\n",
    "    call_ids_abnormal_outcome = calls_with_abnormal_outcome[\"call_id\"].unique()\n",
    "    drop_index = data[data[\"call_id\"].isin(call_ids_abnormal_outcome)].index\n",
    "    data = data.drop(index = drop_index)\n",
    "    \n",
    "    # Focusing only on 1st customer subcalls\n",
    "    data = data[data[\"cust_subcall\"] == 1]\n",
    "    \n",
    "    # Sorting and removing duplicates\n",
    "    data = data.sort_values([\"segment_start\"])\n",
    "    data = data.drop_duplicates(\"call_id\", keep = \"last\")\n",
    "    data = data[(data[\"party_answered\"] > 1000) | (data[\"outcome\"].isin([11,12]))]\n",
    "    \n",
    "    data = data[data[\"service\"] == service]\n",
    "    \n",
    "    # If service is 1, focusing on a specific node\n",
    "    if service == 1:\n",
    "        data = data[data[\"node\"] == node]\n",
    "    \n",
    "    # Recording outcome based on abandonment or not\n",
    "    data.loc[data[\"outcome\"].isin([11,12]),\"abandon\"] = 1 #if abandoned the abandonment time is not censored\n",
    "    data.loc[~data[\"outcome\"].isin([11,12]),\"abandon\"] = 0 #if not abandoned the abandonment time is censored\n",
    "    \n",
    "    return data[[\"call_id\",\"segment_start\",\"queue_time\",\"abandon\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e457313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias corrected Kaplan Meier estimator\n",
    "\n",
    "# Kaplan Meier integrator \n",
    "\n",
    "def kmi(survival_times, censored_flags): \n",
    "    \"\"\"\n",
    "    Calculates the Kaplan-Meier Integrator (KMI) for survival times.\n",
    "\n",
    "    Parameters:\n",
    "    - survival_times: array-like - An array of survival times.\n",
    "    - censored_flags: array-like - A binary array indicating whether each survival time is censored (0) or not (1).\n",
    "\n",
    "    Returns:\n",
    "    - float: The estimated mean survival time.\n",
    "    \"\"\"\n",
    "    if len(survival_times) != len(censored_flags):\n",
    "        raise ValueError(\"Survival times and censored flags must be the same length.\")\n",
    "\n",
    "        \n",
    "    sorted_indices = np.argsort(survival_times)\n",
    "    sorted_times = np.sort(survival_times)\n",
    "    sorted_flags = censored_flags[sorted_indices]\n",
    "    n = len(sorted_flags)\n",
    "\n",
    "    km_weights = np.zeros(n)\n",
    "    km_weights[0] = 1 / n\n",
    "\n",
    "    for i in range(1, n):\n",
    "        km_weights[i] = km_weights[i - 1] * (n - (i+1) + 2)/(n - (i+1) + 1) * (((n - (i+1) + 1)/(n - (i+1) + 2))**sorted_flags[i - 1])\n",
    "\n",
    "    weighted_flags = km_weights * sorted_flags\n",
    "\n",
    "    if sorted_flags[-1] == 0:\n",
    "        weighted_flags[-1] = 1 - np.sum(weighted_flags)\n",
    "\n",
    "    kmi_estimate = np.sum(weighted_flags * sorted_times)\n",
    "    return kmi_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6047bad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bias(survival_times, censored_flags):\n",
    "    \"\"\"\n",
    "    Calculates the bias in the Kaplan-Meier estimator.\n",
    "\n",
    "    Parameters:\n",
    "    - survival_times: array-like - An array of survival times.\n",
    "    - censored_flags: array-like - A binary array indicating whether each survival time is censored (0) or not (1).\n",
    "\n",
    "    Returns:\n",
    "    - float: The calculated bias.\n",
    "    \"\"\"\n",
    "    if len(survival_times) != len(censored_flags):\n",
    "        raise ValueError(\"Survival times and censored flags must be the same length.\")\n",
    "\n",
    "    sorted_indices = np.argsort(survival_times)\n",
    "    sorted_times = np.sort(survival_times)\n",
    "    sorted_flags = censored_flags[sorted_indices]\n",
    "    n = len(sorted_flags)\n",
    "\n",
    "    bias_factors = np.zeros(n - 2)\n",
    "    bias_factors[0] = ((n - 2) / (n - 1)) ** sorted_flags[0]\n",
    "\n",
    "    for i in range(1, n - 2):\n",
    "        bias_factors[i] = bias_factors[i-1]*(((n - (i+1) - 1)/(n - (i+1)))**sorted_flags[i])\n",
    "\n",
    "    bias_value = -(n - 1) / n * sorted_times[-1] * sorted_flags[-1] * (1 - sorted_flags[-2]) * bias_factors[-3]\n",
    "    return bias_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099ce142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jackknife_estimation(survival_times, censored_flags):\n",
    "    \"\"\"\n",
    "    Calculates the bias-corrected Kaplan-Meier estimator using the jackknife method.\n",
    "\n",
    "    Parameters:\n",
    "    - survival_times: array-like - An array of survival times.\n",
    "    - censored_flags: array-like - A binary array indicating whether each survival time is censored (0) or not (1).\n",
    "\n",
    "    Returns:\n",
    "    - float: The bias-corrected Kaplan-Meier estimate.\n",
    "    \"\"\"\n",
    "    kmi_estimate = kmi(survival_times, censored_flags)\n",
    "    bias_estimate = calculate_bias(survival_times, censored_flags)\n",
    "    bias_corrected_estimate = kmi_estimate - bias_estimate\n",
    "    return bias_corrected_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e42a769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def km_estimation(file_list, service, node):\n",
    "    \"\"\"\n",
    "    Concatenates data from multiple files to calculate the mean abandonment times\n",
    "    using a bias-corrected Kaplan-Meier estimator.\n",
    "\n",
    "    Parameters:\n",
    "    - file_list: list of str - List of file paths.\n",
    "    - service: int - Service type to filter.\n",
    "    - node: int - Node number to filter.\n",
    "\n",
    "    Returns:\n",
    "    - float: Bias-corrected Kaplan-Meier estimate of mean abandonment times.\n",
    "    \"\"\"\n",
    "    if not file_list:\n",
    "        raise ValueError(\"File list is empty.\")\n",
    "\n",
    "    main_dataframe = pd.DataFrame(filt_abandon(file_list[0],service,node))\n",
    "    \n",
    "    for i in range(1,len(file_list)):\n",
    "        \n",
    "        data = filt_abandon(file_list[i],service,node)\n",
    "        df = pd.DataFrame(data)\n",
    "        main_dataframe = pd.concat([main_dataframe,df],axis=0)\n",
    "\n",
    "    # Extracting queue times and abandonment flags\n",
    "    queue_times = main_dataframe[\"queue_time\"].to_numpy()\n",
    "    abandonment_flags = main_dataframe[\"abandon\"].to_numpy()\n",
    "\n",
    "    # Calculating the bias-corrected Kaplan-Meier estimate\n",
    "    bias_corrected_estimate = jackknife_estimation(queue_times, abandonment_flags)\n",
    "    \n",
    "    return bias_corrected_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913de116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class names and class indices as they appear in the dataset\n",
    "class_names = [\"Retail_Node1\", \"Retail_Node2\", \"Retail_Node3\", \"Premier\", \"Business\", \"Platinum\", \"Consumer_Loans\", \"Online_Banking\", \"EBO\", \"Telesales\", \"Subanco\", \"Case_Quality\", \"Priority_Service\", \"AST\", \"CCO\", \"Brokerage\", \"BPS\"]\n",
    "service_class_indices = [1, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 17]\n",
    "\n",
    "# Create a DataFrame to store the abandonment times\n",
    "abandonment_times_17dim = pd.DataFrame(index=class_names, columns=[\"Total\"])\n",
    "\n",
    "# Calculate and save abandonment times for each class\n",
    "for i, class_name in enumerate(class_names):\n",
    "    \n",
    "    node = i + 1 if i < 3 else 0  # First three are retail classes with specific nodes\n",
    "    abandonment_times_17dim.loc[class_name, \"Total\"] = km_estimation(file_list_cust_subcalls, service_class_indices[i], node) #abandonment times in second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353b461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving abandonment times in seconds and abandonment rates per hour into csv files\n",
    "file_name_abandonment_times = os.path.join(main_test_folder_path, f\"abandonment_times_{class_no}dim.csv\")\n",
    "np.savetxt(file_name_abandonment_times, abandonment_times_17dim, delimiter = \",\")\n",
    "\n",
    "# hourly abandonment rates\n",
    "file_name_hourly_theta = os.path.join(main_test_folder_path, f\"theta_hourly_{class_no}dim.csv\")\n",
    "np.savetxt(file_name_hourly_theta, SECONDS_IN_HOUR/abandonment_times_17dim, delimiter = \",\", fmt=\"%.2f\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce14386",
   "metadata": {},
   "source": [
    "### 1.4 Cost rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9d894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We impute holding cost data based on relative importance of the classes and their call volume\n",
    "# please refer to the paper on how the holding costs are determined\n",
    "\n",
    "holding_cost_17dim = np.array([24,24,24,26,30,32,22,22,20,22,20,20,32,22,22,22,20]) #hourly holding cost\n",
    "abandonment_cost_17dim = holding_cost_17dim/12 #abandonment penalty \n",
    "\n",
    "file_name_holding_cost = os.path.join(main_test_folder_path, f\"hourly_holding_cost_{class_no}dim.csv\")\n",
    "file_name_abandonment_cost = os.path.join(main_test_folder_path,f\"abandonment_cost_{class_no}dim.csv\")\n",
    "\n",
    "np.savetxt(file_name_holding_cost, holding_cost_17dim, delimiter = \",\", fmt=\"%.2f\")\n",
    "np.savetxt(file_name_abandonment_cost, abandonment_cost_17dim, delimiter = \",\", fmt=\"%.2f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e7159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total cost rate calculation -- per hour\n",
    "theta_hourly = SECONDS_IN_HOUR / abandonment_times_17dim[\"Total\"].to_numpy()\n",
    "total_costs = holding_cost_17dim + theta_hourly * abandonment_cost_17dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c522966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving total cost rates per hour\n",
    "file_name_total_cost = os.path.join(main_test_folder_path, f\"hourly_total_cost_{class_no}dim.csv\")\n",
    "np.savetxt(file_name_total_cost, total_costs, delimiter = \",\", fmt=\"%.2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d844af0",
   "metadata": {},
   "source": [
    "### 1.5 Arrival Percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046ab0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_rate(file_path,service,node):  \n",
    "    \"\"\"\n",
    "    Calculates the daily rate of calls for a specific service and node.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str - Path to the data file.\n",
    "    - service: int - Service type to filter.\n",
    "    - node: int - Node number to filter.\n",
    "\n",
    "    Returns:\n",
    "    - int: Number of calls after filtering.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "    except Exception as e:\n",
    "        raise FileNotFoundError(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "    #eliminating outliers through distribution plot of queueing times and service times\n",
    "    data = data[data[\"queue_time\"] < 900] #removing outliers based on the distribution\n",
    "    data = data[data[\"service_time\"] < 1800] #remove based on the service times\n",
    "    \n",
    "    #removing the calls with abnormal outcomes \n",
    "    abnormal_outcomes = [4,13,14,23,30,40,50]\n",
    "    calls_with_abnormal_outcome = data[data[\"outcome\"].isin(abnormal_outcomes)]\n",
    "    call_ids_abnormal_outcome = calls_with_abnormal_outcome[\"call_id\"].unique()\n",
    "    drop_index = data[data[\"call_id\"].isin(call_ids_abnormal_outcome)].index\n",
    "    data = data.drop(index = drop_index)\n",
    "    \n",
    "    #focusing only on 1st customer subcalls\n",
    "    data = data[data[\"cust_subcall\"] == 1]\n",
    "    \n",
    "    data = data.sort_values([\"segment_start\"])\n",
    "    data = data.drop_duplicates(\"call_id\",keep = \"last\") #removing the multiple records of the same call\n",
    "        \n",
    "    if service == 1:\n",
    "        data = data[data[\"service\"] == 1]\n",
    "        data = data[data[\"node\"].isin([node])]\n",
    "    else:\n",
    "        data = data[data[\"service\"] == service]\n",
    "        \n",
    "    return len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9599760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_rate_means(file_list, service, node):\n",
    "    \"\"\"\n",
    "    Calculates the mean daily rate for a given service and node across multiple files.\n",
    "\n",
    "    Parameters:\n",
    "    - file_list: list of str - List of file paths.\n",
    "    - service: int - Service type to filter.\n",
    "    - node: int - Node number to filter.\n",
    "\n",
    "    Returns:\n",
    "    - float: Mean daily rate.\n",
    "    \"\"\"\n",
    "    total_calls = sum(daily_rate(file, service, node) for file in file_list)\n",
    "    return total_calls / len(file_list) if file_list else 0\n",
    "\n",
    "# DataFrame to store the daily rates\n",
    "class_indices = [1, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 17]\n",
    "d_rates = pd.DataFrame({'index': class_indices, 'Total': 0})\n",
    "\n",
    "# Calculate the mean daily rates for each class\n",
    "for i, index in enumerate(d_rates['index']):\n",
    "    node = i + 1 if i < 3 else 0  # First three are for specific nodes\n",
    "    d_rates.loc[i, 'Total'] = daily_rate_means(file_list_cust_subcalls, index, node)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e987f30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = np.array(d_rates[\"Total\"])\n",
    "percentages_17dim = observations/np.sum(observations)\n",
    "file_name_percentage = os.path.join(main_test_folder_path, f\"percentages_{class_no}dim.csv\")\n",
    "np.savetxt(file_name_percentage, percentages_17dim*100, delimiter=\",\", fmt=\"%.2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2c6508",
   "metadata": {},
   "source": [
    "### 1.6 Cumulative distribution function of arrivals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd93f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to calculate the cdf of arrivals to use in our discrete event simulation to determine which class arrives\n",
    "# 204 time intervals (every interval is 5 minute), 17 different classes\n",
    "results = np.zeros((length_time, class_no))\n",
    "\n",
    "# Populate the results array with arrival data for each class\n",
    "for i, key in enumerate(results_by_class.keys()):\n",
    "    results[:, i] = results_by_class[key][time_focus_start:time_focus_end]\n",
    "\n",
    "# Calculate cumulative sum of arrivals for each time interval\n",
    "cumsum = np.cumsum(results, axis=1)\n",
    "\n",
    "# Reshape total arrivals for broadcasting in division\n",
    "total_arrivals_reshaped  = np.array(cumsum[:,class_no-1]).reshape(-1, 1)\n",
    "\n",
    "# Check if total arrivals are zero to prevent division by zero\n",
    "if np.any(total_arrivals_reshaped == 0):\n",
    "    raise ValueError(\"Total arrivals contain zero values, cannot calculate CDF.\")\n",
    "\n",
    "# Calculate CDF by normalizing cumulative sums with total arrivals\n",
    "cdf = cumsum / total_arrivals_reshaped\n",
    "\n",
    "# Save the CDF to a CSV file\n",
    "file_name_cdf = os.path.join(main_test_folder_path, f\"cdf_{class_no}dim.csv\")\n",
    "np.savetxt(file_name_cdf, cdf, delimiter=\",\", fmt=\"%.3f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9637ca",
   "metadata": {},
   "source": [
    "### 1.7 Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec61f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list_agent_events = np.sort(get_file_list(data_folder_path,\"*_agent_events.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4422a514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reader(file_path):\n",
    "    \"\"\"\n",
    "    Reads the specified file and returns the unique IDs of calls with a second subcall.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str - Path to the data file.\n",
    "\n",
    "    Returns:\n",
    "    - ndarray: Array of unique IDs for calls with a second subcall.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "    except Exception as e:\n",
    "        raise FileNotFoundError(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "    # Extracting unique IDs of calls with a second subcall\n",
    "    second_subcall_ids = data[data[\"cust_subcall\"] == 2][\"record_id\"].unique()\n",
    "    return second_subcall_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8403b341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting unique IDs from all files\n",
    "file_list_agent_records_sorted = np.sort(file_list_agent_records)\n",
    "data_sets = [reader(file) for file in file_list_agent_records_sorted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad72870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seconds(array: Union[List[timedelta], pd.Series]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Converts an array-like of timedelta objects to an array of seconds.\n",
    "\n",
    "    Parameters:\n",
    "    - array (Union[List[timedelta], pd.Series]): An array-like structure of timedelta objects.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: An array of seconds corresponding to the timedelta objects.\n",
    "    \"\"\"\n",
    "    if not all(isinstance(td, timedelta) for td in array):\n",
    "        raise ValueError(\"All elements of the array must be timedelta objects\")\n",
    "\n",
    "    if isinstance(array, pd.Series):\n",
    "        # Vectorized operation for pandas Series\n",
    "        return array.dt.seconds.to_numpy()\n",
    "    else:\n",
    "        # List comprehension for a list\n",
    "        return np.array([td.seconds for td in array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afafd7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filt_agents(file_path,precision,num):\n",
    "    \"\"\"\n",
    "    Calculates the average availability of agents over specified time intervals.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the CSV file containing the data.\n",
    "    - precision (int): Time precision for averaging availability.\n",
    "    - num (int): Identifier for data set selection.\n",
    "    - event_ids (dict): Dictionary of event IDs for different categories.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Array of average agent availability.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "    except Exception as e:\n",
    "        raise FileNotFoundError(f\"Error reading the file: {e}\")\n",
    "\n",
    "    \n",
    "    #sign-in times\n",
    "    data_sign_in = data[((data[\"event_id\"] == 20) | (data[\"event_id\"] == 21))] #events 20 and 21 are sign-in times\n",
    "    data_sign_in = data_sign_in.sort_values([\"agent\",\"event_start\"])\n",
    "    data_sign_in = data_sign_in.reset_index()\n",
    "    sign_in = pd.to_timedelta(data_sign_in[\"event_start\"], unit = 'seconds')\n",
    "    sign_in_times = seconds(sign_in) #converting sign_in times to seconds\n",
    "    \n",
    "    #sign_out times\n",
    "    data_sign_out = data[((data[\"event_id\"] == 30) | (data[\"event_id\"] == 31))] #events 30 and 31 are sign-out times\n",
    "    data_sign_out = data_sign_out.sort_values([\"agent\",\"event_start\"])\n",
    "    data_sign_out = data_sign_out.reset_index()\n",
    "    sign_out = pd.to_timedelta(data_sign_out[\"event_start\"], unit = 'seconds')\n",
    "    sign_out_times = seconds(sign_out) #converting sign_out times to seconds\n",
    "    \n",
    "    available = np.zeros(86400)\n",
    "    for i in range(len(data_sign_in)):\n",
    "        if sign_in_times[i] > sign_out_times[i]:\n",
    "            sign_out_times[i] = 86399\n",
    "        for j in range(int(sign_in_times[i]),int(sign_out_times[i])+1):\n",
    "            available[int(j)] += 1\n",
    "    #breaks\n",
    "    data_break = data[((data[\"event_id\"] == 60) | (data[\"event_id\"] == 61) | (data[\"event_id\"] == 62))] #events 60, 61,62 are break times\n",
    "    data_break = data_break.sort_values([\"agent\",\"event_start\"])\n",
    "    data_break = data_break.reset_index()\n",
    "    \n",
    "    break_start = pd.to_timedelta(data_break[\"event_start\"], unit = 'seconds')\n",
    "    break_end = pd.to_timedelta(data_break[\"event_end\"], unit = 'seconds')\n",
    "    \n",
    "    break_start = seconds(break_start)\n",
    "    break_end = seconds(break_end)\n",
    "    \n",
    "    for i in range(len(break_start)):\n",
    "        for j in range(int(break_start[i]),int(break_end[i]) + 1):\n",
    "            available[int(j)] -= 1\n",
    "            \n",
    "    #second subcalls eliminated\n",
    "    data_ss = data[(data[\"record_id\"].isin(data_sets[num]))]\n",
    "    data_ss = data_ss.sort_values([\"agent\", \"event_start\"])\n",
    "    data_ss = data_ss.reset_index()\n",
    "    \n",
    "    ss_start = pd.to_timedelta(data_ss[\"event_start\"], unit = \"seconds\")\n",
    "    ss_end = pd.to_timedelta(data_ss[\"event_end\"], unit = \"seconds\")\n",
    "    \n",
    "    ss_start = seconds(ss_start)\n",
    "    ss_end = seconds(ss_end)\n",
    "    \n",
    "    for k in range(len(ss_start)):\n",
    "        for l in range(int(ss_start[k]), int(ss_end[k]) + 1):\n",
    "            available[int(l)] -= 1\n",
    "    \n",
    "    available = np.reshape(available, (-1,precision*60))\n",
    "    \n",
    "    mean = np.mean(available, axis = 1) #mean\n",
    "    \n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b795f6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_agent_availability(file_list, precision):\n",
    "    \"\"\"\n",
    "    Calculates the average agent availability across multiple data files.\n",
    "\n",
    "    Parameters:\n",
    "    - file_list (list of str): List of file paths to process.\n",
    "    - precision (int): Time precision parameter to pass to the filt_agents function.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: The average availability of agents across the given files.\n",
    "    \"\"\"\n",
    "    \n",
    "    total_mean = 0\n",
    "    for i, file in enumerate(file_list):\n",
    "        mean = filt_agents(file, precision, i)\n",
    "        total_mean += mean\n",
    "\n",
    "    average_availability = total_mean / len(file_list)\n",
    "\n",
    "    return average_availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbae3c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = np.array(calculate_average_agent_availability(file_list_agent_events,precision))\n",
    "agents = agents.round().astype(int)[time_focus_start:time_focus_end] #focus on the period between 7 am till midnight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c6ddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_agents = os.path.join(main_test_folder_path, f\"main_test_agents.csv\")\n",
    "np.savetxt(file_name_agents, agents, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec999813",
   "metadata": {},
   "source": [
    "### 1.8 Limiting arrival process and zeta calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bfb589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_limiting_hourly_rates(class_no, lambda_prelimit, time_focus_start, time_focus_end, precision, mu_hourly, percentages, agents, length_time, SCALING_FACTOR):\n",
    "    \"\"\"\n",
    "    Computes hourly rates for given parameters and class information.\n",
    "\n",
    "    Parameters:\n",
    "    - class_no (int): Number of classes.\n",
    "    - lambda_prelimit: Array of prelimit lambda by class.\n",
    "    - time_focus_start (int): Start index for focusing time.\n",
    "    - time_focus_end (int): End index for focusing time.\n",
    "    - precision (int): Precision value for time calculations.\n",
    "    - mu_hourly (np.array): Array of hourly service rates.\n",
    "    - percentages (np.array): Array of percentages.\n",
    "    - agents (int): Number of agents.\n",
    "    - length_time (int): Length of the time array.\n",
    "    - SCALING_FACTOR (int): Factor to scale the system.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Tuple containing lambda limit hourly, and zeta hourly arrays.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculations\n",
    "    denominator_hourly = SCALING_FACTOR * np.sum(percentages / mu_hourly)\n",
    "\n",
    "    lambd_limit_hourly = np.zeros((class_no, length_time))\n",
    "    zeta_hourly = np.zeros((class_no, length_time))\n",
    "\n",
    "    for i in range(class_no):\n",
    "        lambd_limit_hourly[i, :] = percentages[i] * agents / denominator_hourly\n",
    "        zeta_hourly[i, :] = (lambda_prelimit[i, :] - SCALING_FACTOR * lambd_limit_hourly[i, :]) / np.sqrt(SCALING_FACTOR)\n",
    "\n",
    "    return lambd_limit_hourly, zeta_hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c847b6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambd_prelimit_hourly = np.zeros((class_no, length_time)) #hourly prelimit arrival rate\n",
    "mu_hourly = SECONDS_IN_HOUR / service_times_17dim[\"Total\"]\n",
    "\n",
    "for i, key in enumerate(results_by_class.keys()):\n",
    "    lambd_prelimit_hourly[i, :] = results_by_class[key][time_focus_start:time_focus_end]*MINUTES_IN_HOUR/precision\n",
    "\n",
    "lambd_limit_hourly, zeta_hourly = compute_limiting_hourly_rates(class_no, lambd_prelimit_hourly, time_focus_start, time_focus_end, precision, mu_hourly, percentages_17dim, agents, length_time, SCALING_FACTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b79d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_lambda_limiting = os.path.join(main_test_folder_path, f\"main_test_hourly_limiting_lambda.csv\")\n",
    "file_name_zeta = os.path.join(main_test_folder_path, f\"main_test_hourly_zeta.csv\")\n",
    "\n",
    "np.savetxt(file_name_lambda_limiting, lambd_limit_hourly, delimiter = \",\")\n",
    "np.savetxt(file_name_zeta, zeta_hourly, delimiter = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9dd6c1",
   "metadata": {},
   "source": [
    "## 2. Low Dimensional Test Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c3154e",
   "metadata": {},
   "source": [
    "### 2.1 Pre-limit arrival process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23365f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filt_arrivals_low_dim(file_path,precision,service_lookup,service,class_no):\n",
    "    \"\"\"\n",
    "    Processes and filters call arrival data based on various criteria.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the CSV file containing the data.\n",
    "    - precision (int): Time precision for resampling.\n",
    "    - service_lookup (bool): Whether to filter data based on service lookup.\n",
    "    - service_class (int): The service class to filter.\n",
    "    - class_no (int): Number of classes.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Filtered and processed data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "    except Exception as e:\n",
    "        raise FileNotFoundError(f\"Error reading the file: {e}\")\n",
    "    \n",
    "    \n",
    "    data = data[data[\"queue_time\"] < 900] #removing outliers based on the distribution of queueing times\n",
    "    data = data[data[\"service_time\"] < 1800] #removing outliers based on the distribution of service times\n",
    "    \n",
    "    #removing the calls with abnormal outcomes \n",
    "    abnormal_outcomes = [4,13,14,23,30,40,50]\n",
    "    calls_with_abnormal_outcome = data[data[\"outcome\"].isin(abnormal_outcomes)]\n",
    "    call_ids_abnormal_outcome = calls_with_abnormal_outcome[\"call_id\"].unique()\n",
    "    drop_index = data[data[\"call_id\"].isin(call_ids_abnormal_outcome)].index\n",
    "    data = data.drop(index = drop_index)\n",
    "    \n",
    "    #focusing only on 1st customer subcalls\n",
    "    data = data[data[\"cust_subcall\"] == 1]\n",
    "    \n",
    "    #removing the multiple records of the same call\n",
    "    data = data.sort_values([\"segment_start\"])\n",
    "    data = data.drop_duplicates(\"call_id\",keep = \"last\")\n",
    "    \n",
    "\n",
    "    if service_lookup == True:\n",
    "        if class_no == 2:\n",
    "            if service == 1:\n",
    "                class1_codes = [1,3,5,6,8,13]\n",
    "                data = data[data[\"service\"].isin(class1_codes)]\n",
    "                node_drop_index = data[(data[\"service\"] == 1) & (data[\"node\"].isin([1,3,5,6,7]))].index #keep only node 2\n",
    "                #drop nodes 1,3,5,6,7\n",
    "                data = data.drop(index = node_drop_index) \n",
    "\n",
    "            elif service == 2:\n",
    "                class2_codes = [1,2,4,7,9,10,11,12,15,17] \n",
    "                data = data[data[\"service\"].isin(class2_codes)]\n",
    "                #drop nodes 2,5,6,7\n",
    "                node_drop_index = data[(data[\"service\"] == 1) & (data[\"node\"].isin([2,5,6,7]))].index #keep only nodes 1,3\n",
    "                data = data.drop(index = node_drop_index)\n",
    "        \n",
    "        elif class_no == 3:\n",
    "            \n",
    "            if service == 1:\n",
    "                class1_codes = [1,3,8]\n",
    "                data = data[data[\"service\"].isin(class1_codes)]\n",
    "                node_drop_index = data[(data[\"service\"] == 1) & (data[\"node\"].isin([1,3,5,6,7]))].index #keep only node 2\n",
    "                #drop nodes 1,3,5,6,7\n",
    "                data = data.drop(index = node_drop_index) \n",
    "        \n",
    "            elif service == 2:\n",
    "                class2_codes = [1,5,6,13] \n",
    "                data = data[data[\"service\"].isin(class2_codes)]\n",
    "                #drop nodes 2,5,6,7\n",
    "                node_drop_index = data[(data[\"service\"] == 1) & (data[\"node\"].isin([2,3,5,6,7]))].index #keep only nodes 1\n",
    "                data = data.drop(index = node_drop_index)\n",
    "        \n",
    "            elif service == 3:\n",
    "                class3_codes = [1,2,4,7,9,10,11,12,15,17]\n",
    "                data = data[data[\"service\"].isin(class3_codes)]\n",
    "                node_drop_index = data[(data[\"service\"] == 1) & (data[\"node\"].isin([1,2,5,6,7]))].index #keep only node 3\n",
    "                data = data.drop(index = node_drop_index)\n",
    "            \n",
    "    #some calls when they first arrive are on hold and then are transferred to the agents but since they have the same call id we look at their first occurrence\n",
    "    data = data.reset_index()\n",
    "    \n",
    "    #defining the start time of the events\n",
    "    start = []\n",
    "    for i in range(len(data)):\n",
    "        start.append(datetime.fromtimestamp(data[\"segment_start\"][i]+5*60*60).strftime(\"%A, %B %d, %Y %H:%M:%S\")) \n",
    "    data[\"start\"] = start\n",
    "    data['Datetime'] = pd.to_datetime(data['start'])\n",
    "    data[\"day\"] = pd.to_datetime(start).date\n",
    "    new_data = data\n",
    "    drop_ind = new_data.drop_duplicates(subset = [\"day\"])\n",
    "    \n",
    "    if len(drop_ind) > 1:\n",
    "        drop_day = np.array(drop_ind.day)[1]\n",
    "        data = data[data[\"day\"] != drop_day]\n",
    "    \n",
    "    data = data.set_index('Datetime')\n",
    "    \n",
    "    #resolution\n",
    "    resolution = \"{pre}T\"\n",
    "    \n",
    "    #how many arrivals happened within the specific time interval\n",
    "    data = data.resample(resolution.format(pre = precision)).count()\n",
    "    data = data.dropna(axis = 0)\n",
    "    \n",
    "    #setting up a new column to show the time interval\n",
    "    new_index = []\n",
    "    for j in data.index:\n",
    "        h=j.hour\n",
    "        m=j.minute\n",
    "        item=str(h)+':'+str(m)\n",
    "        new_index.append(item)\n",
    "        \n",
    "    data[\"index\"] = new_index\n",
    "    data = data.reset_index()\n",
    "    \n",
    "    return data[[\"index\",\"call_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8800e3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_daily_data_low_dim(file_list, precision, service_lookup, service, class_no):\n",
    "    \"\"\"\n",
    "    Merges and processes data from multiple days.\n",
    "\n",
    "    Parameters:\n",
    "    - file_list: list of str - List of file paths.\n",
    "    - precision: int - Time precision for resampling.\n",
    "    - service_lookup: bool - Whether to filter by service.\n",
    "    - service: int - Service type to filter.\n",
    "    - class_no (int): Number of classes.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: Merged and processed data.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not file_list:\n",
    "        raise ValueError(\"The file list is empty.\")\n",
    "        \n",
    "    time_intervals = create_time_intervals()\n",
    "    main_dataframe = pd.DataFrame(index=pd.Index(time_intervals))\n",
    "    main_dataframe = main_dataframe.merge(filt_arrivals_low_dim(file_list[0],precision,service_lookup,service,class_no), how = \"left\", left_on = main_dataframe.index, right_on = \"index\", suffixes=('', '_0'))\n",
    "    main_dataframe = main_dataframe.fillna(0)\n",
    "    \n",
    "    for i in range(1,len(file_list)):\n",
    "        #merging all the days based on their time arrival \n",
    "        main_dataframe = main_dataframe.merge(filt_arrivals_low_dim(file_list[i],precision,service_lookup,service,class_no), how = \"left\", left_on = \"index\", right_on = \"index\", suffixes=('', f'_{i}'))\n",
    "    \n",
    "    #dropping the null values\n",
    "    main_dataframe = main_dataframe.fillna(0)\n",
    "    \n",
    "    #setting the index to show the arrival intervals\n",
    "    main_dataframe = main_dataframe.set_index(\"index\")\n",
    "    \n",
    "    \n",
    "    return main_dataframe\n",
    "        \n",
    "\n",
    "def means_totals_low_dim(file_list, precision, service_lookup, service, class_no):\n",
    "    \"\"\"\n",
    "    Calculates the average arrival rate over multiple days.\n",
    "\n",
    "    Parameters:\n",
    "    - file_list: list of str - List of file paths.\n",
    "    - precision: int - Time precision for resampling.\n",
    "    - service_lookup: bool - Whether to filter by service.\n",
    "    - service: int - Service type to filter.\n",
    "    - class_no (int): Number of classes.\n",
    "    \n",
    "    Returns:\n",
    "    - Series: Average arrival rate for each time interval.\n",
    "    \"\"\"\n",
    "    merged_data = merge_daily_data_low_dim(file_list, precision, service_lookup, service, class_no)\n",
    "    \n",
    "    return merged_data.mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd60366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv_low_dim(folder_path, file_list, class_indices, class_no, precision, time_focus_start, time_focus_end):\n",
    "    \"\"\"\n",
    "    Saves results for each class into CSV files in the specified directory.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path: str - Folder path to save the arrivals\n",
    "    - file_list: list of str - List of file paths.\n",
    "    - class_indices (list): List of class indices.\n",
    "    - class_no (int): Number of classes.\n",
    "    - precision (int): Precision used in calculations.\n",
    "    - time_focus_start (int): Start index for focused time.\n",
    "    - time_focus_end (int): End index for focused time.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Dictionary of results by class.\n",
    "    \"\"\"\n",
    "    \n",
    "    results_by_class = {}\n",
    "    \n",
    "    for class_ind in range(len(class_indices)):\n",
    "        \n",
    "        results_by_class[f\"results_class{class_ind+1}_{class_no}dim\"] = means_totals_low_dim(file_list, precision, service_lookup=True, service=class_ind + 1, class_no=class_no)\n",
    "        \n",
    "        file_name_all = os.path.join(folder_path, f\"arrivals_class{class_ind+1}_all_{precision}min_{class_no}dim.csv\")\n",
    "        file_name_partial = os.path.join(folder_path, f\"arrivals_class{class_ind+1}_partial_{precision}min_{class_no}dim.csv\")\n",
    "        \n",
    "        np.savetxt(file_name_all, results_by_class[f\"results_class{class_ind+1}_{class_no}dim\"], delimiter=\",\")\n",
    "        np.savetxt(file_name_partial, results_by_class[f\"results_class{class_ind+1}_{class_no}dim\"][time_focus_start:time_focus_end], delimiter=\",\")  \n",
    "    \n",
    "    return results_by_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00bb1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 is Retail (node: 1)\n",
    "# 1 is Retail (node: 2)\n",
    "# 2 is Retail (node: 3)\n",
    "\n",
    "# 2 dimensional problem\n",
    "class_no = 2\n",
    "class_indices_2dim = [[1, 4, 6, 7, 9, 14], [0, 2, 3, 5, 8, 10, 11, 12, 13, 15, 16]]\n",
    "\n",
    "folder_path_2dim = os.path.join(data_folder_path,f\"problem_{class_no}dim\")\n",
    "os.makedirs(folder_path_2dim, exist_ok=True)\n",
    "prelimit_arrivals_2dim = save_to_csv_low_dim(folder_path_2dim, file_list_cust_subcalls, class_indices_2dim, 2, precision, time_focus_start, time_focus_end)\n",
    "\n",
    "# 3 dimensional problem\n",
    "class_no = 3\n",
    "class_indices_3dim = [[1, 4, 9], [0, 6, 7, 14], [2, 3, 5, 8, 10, 11, 12, 13, 15, 16]]\n",
    "\n",
    "folder_path_3dim = os.path.join(data_folder_path,f\"problem_{class_no}dim\")\n",
    "os.makedirs(folder_path_3dim, exist_ok=True)\n",
    "prelimit_arrivals_3dim = save_to_csv_low_dim(folder_path_3dim, file_list_cust_subcalls, class_indices_3dim, 3, precision, time_focus_start, time_focus_end)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992feccc",
   "metadata": {},
   "source": [
    "### 2.2 System parameters (Service, Abandonment and Cost rates) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd7d67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_dim_parameters(folder_path, class_indices,class_no):\n",
    "    \"\"\"\n",
    "    Computes low dimensional parameters based on class indices and number of classes.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path: str - Folder path to save the system parameters\n",
    "    - class_indices (list): Indices of the classes.\n",
    "    - class_no (int): Number of classes.\n",
    "    \n",
    "    Returns:\n",
    "    - tuple: Returns mu_low, theta_low, cost_low, and percentages_low arrays.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(class_indices, list) or not isinstance(class_no, int):\n",
    "        raise ValueError(\"Invalid input types for class_indices and class_no\")\n",
    "\n",
    "    percentages_low = np.zeros(class_no)\n",
    "    mu_low = np.zeros(class_no)\n",
    "    theta_low = np.zeros(class_no)\n",
    "    holding_cost_low = np.zeros(class_no)\n",
    "    abandonment_cost_low = np.zeros(class_no)\n",
    "    cost_low = np.zeros(class_no)\n",
    "    \n",
    "    # Calculate percentages\n",
    "    for i in range(class_no):\n",
    "        percentages_low[i] = np.sum(percentages_17dim[class_indices[i]])\n",
    "    \n",
    "    # Calculate other system parameters (per hour)\n",
    "    for i, indices in enumerate(class_indices):\n",
    "        percentages_ratio = percentages_17dim[indices] / percentages_low[i]\n",
    "        mu_low[i] = np.sum(SECONDS_IN_HOUR / service_times_17dim[\"Total\"][indices] * percentages_ratio)\n",
    "        theta_low[i] = np.sum(SECONDS_IN_HOUR / abandonment_times_17dim[\"Total\"][indices] * percentages_ratio)\n",
    "        holding_cost_low[i] = np.sum(holding_cost_17dim[indices] * percentages_ratio)\n",
    "        abandonment_cost_low[i] = np.sum(abandonment_cost_17dim[indices] * percentages_ratio)\n",
    "        cost_low[i] = holding_cost_low[i] + theta_low[i] * abandonment_cost_low[i]\n",
    "    \n",
    "    # Save outputs to CSV\n",
    "    file_name_hourly_mu = os.path.join(folder_path, f\"mu_hourly_{class_no}dim.csv\")\n",
    "    file_name_hourly_theta = os.path.join(folder_path, f\"theta_hourly_{class_no}dim.csv\")\n",
    "\n",
    "    file_name_holding_cost = os.path.join(folder_path, f\"hourly_holding_cost_{class_no}dim.csv\")\n",
    "    file_name_abandonment_cost = os.path.join(folder_path,f\"abandonment_cost_{class_no}dim.csv\")\n",
    "    file_name_total_cost = os.path.join(folder_path, f\"hourly_total_cost_{class_no}dim.csv\")\n",
    "    file_name_percentage = os.path.join(folder_path, f\"percentages_{class_no}dim.csv\")\n",
    "    \n",
    "    np.savetxt(file_name_hourly_mu, mu_low, delimiter = \",\", fmt=\"%.2f\")\n",
    "    np.savetxt(file_name_hourly_theta, theta_low, delimiter = \",\", fmt=\"%.2f\")\n",
    "    \n",
    "    np.savetxt(file_name_abandonment_cost, abandonment_cost_low, delimiter = \",\", fmt=\"%.2f\") \n",
    "    np.savetxt(file_name_holding_cost, holding_cost_low, delimiter = \",\", fmt=\"%.2f\")\n",
    "    np.savetxt(file_name_total_cost, cost_low, delimiter = \",\", fmt=\"%.2f\") \n",
    "    np.savetxt(file_name_percentage, percentages_low, delimiter = \",\", fmt=\"%.2f\") \n",
    "    \n",
    "    return mu_low, theta_low, cost_low, percentages_low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdba1c91",
   "metadata": {},
   "source": [
    "### 2.3 Limiting arrival rates and zeta calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fce54ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 dim test problem parameters\n",
    "class_no = 2\n",
    "\n",
    "lambd_prelimit_hourly_2dim = np.zeros((class_no, length_time)) #hourly prelimit arrival rate\n",
    "mu_hourly_2dim, theta_hourly_2dim, cost_hourly_2dim, percentages_2dim = low_dim_parameters(folder_path_2dim,class_indices_2dim,class_no)\n",
    "\n",
    "for i, key in enumerate(prelimit_arrivals_2dim.keys()):\n",
    "    lambd_prelimit_hourly_2dim[i, :] = prelimit_arrivals_2dim[key][time_focus_start:time_focus_end]*MINUTES_IN_HOUR/precision\n",
    "\n",
    "lambd_limit_hourly_2dim, zeta_hourly_2dim = compute_limiting_hourly_rates(class_no, lambd_prelimit_hourly_2dim, time_focus_start, time_focus_end, precision, mu_hourly_2dim, percentages_2dim, agents, length_time, SCALING_FACTOR)\n",
    "\n",
    "\n",
    "file_name_lambda_limiting = os.path.join(folder_path_2dim, f\"hourly_limiting_lambda_{class_no}dim.csv\")\n",
    "file_name_zeta = os.path.join(folder_path_2dim, f\"hourly_zeta_{class_no}dim.csv\")\n",
    "\n",
    "np.savetxt(file_name_lambda_limiting, lambd_limit_hourly_2dim, delimiter = \",\")\n",
    "np.savetxt(file_name_zeta, zeta_hourly_2dim, delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5850d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 dim test problem parameters\n",
    "class_no = 3\n",
    "\n",
    "lambd_prelimit_hourly_3dim = np.zeros((class_no, length_time)) #hourly prelimit arrival rate\n",
    "mu_hourly_3dim, theta_hourly_3dim, cost_hourly_3dim, percentages_3dim = low_dim_parameters(folder_path_3dim, class_indices_3dim,class_no)\n",
    "\n",
    "for i, key in enumerate(prelimit_arrivals_3dim.keys()):\n",
    "    lambd_prelimit_hourly_3dim[i, :] = prelimit_arrivals_3dim[key][time_focus_start:time_focus_end]*MINUTES_IN_HOUR/precision\n",
    "\n",
    "lambd_limit_hourly_3dim, zeta_hourly_3dim = compute_limiting_hourly_rates(class_no, lambd_prelimit_hourly_3dim, time_focus_start, time_focus_end, precision, mu_hourly_3dim, percentages_3dim, agents, length_time, SCALING_FACTOR)\n",
    "\n",
    "file_name_lambda_limiting = os.path.join(folder_path_3dim, f\"hourly_limiting_lambda_{class_no}dim.csv\")\n",
    "file_name_zeta = os.path.join(folder_path_3dim, f\"hourly_zeta_{class_no}dim.csv\")\n",
    "\n",
    "np.savetxt(file_name_lambda_limiting, lambd_limit_hourly_3dim, delimiter = \",\")\n",
    "np.savetxt(file_name_zeta, zeta_hourly_3dim, delimiter = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8cd47d",
   "metadata": {},
   "source": [
    "### 2.4 Cumulative distribution function of arrivals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab5fd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to calculate the cdf of arrivals to use in our discrete event simulation to determine which class arrives\n",
    "class_no = 2\n",
    "\n",
    "# Calculate cumulative sum of arrivals for each time interval\n",
    "cumsum_2dim = np.cumsum(np.transpose(lambd_prelimit_hourly_2dim), axis=1)\n",
    "total_arrivals_reshaped  = np.array(cumsum_2dim[:,class_no-1]).reshape(-1, 1)\n",
    "\n",
    "# Calculate CDF by normalizing cumulative sums with total arrivals\n",
    "cdf_2dim = cumsum_2dim / total_arrivals_reshaped\n",
    "\n",
    "# Save the CDF to a CSV file\n",
    "file_name_cdf = os.path.join(folder_path_2dim, f\"cdf_{class_no}dim.csv\")\n",
    "np.savetxt(file_name_cdf, cdf_2dim, delimiter=\",\", fmt=\"%.3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d08943",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_no = 3\n",
    "\n",
    "# Calculate cumulative sum of arrivals for each time interval\n",
    "cumsum_3dim = np.cumsum(np.transpose(lambd_prelimit_hourly_3dim), axis=1)\n",
    "total_arrivals_reshaped  = np.array(cumsum_3dim[:,class_no-1]).reshape(-1, 1)\n",
    "\n",
    "# Calculate CDF by normalizing cumulative sums with total arrivals\n",
    "cdf_3dim = cumsum_3dim / total_arrivals_reshaped\n",
    "\n",
    "# Save the CDF to a CSV file\n",
    "file_name_cdf = os.path.join(folder_path_3dim, f\"cdf_{class_no}dim.csv\")\n",
    "np.savetxt(file_name_cdf, cdf_3dim, delimiter=\",\", fmt=\"%.3f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fa3361",
   "metadata": {},
   "source": [
    "## 3. High Dimensional Test Problems "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171e5d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_test_class_no = 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9216666",
   "metadata": {},
   "source": [
    "### 3.1 Pre-limit arrival process and system parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55902f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_high_dimensional_parameters(folder_path, high_dim_class_no, main_test_class_no, arrivals_main, mu_hourly_main, theta_hourly_main, agents, old_class_no, old_scaling_factor, precision, seed_value):\n",
    "    \"\"\"\n",
    "    Calculate and save various high-dimensional parameters.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path (str): Folder path to save parameters.\n",
    "    - high_dim_class_no (int): Number of high-dimensional classes.\n",
    "    - main_test_class_no (int): Number of main test classes.\n",
    "    - arrivals_main (np.ndarray): Main array of arrivals.\n",
    "    - mu_hourly_main (np.ndarray): Main hourly service rates.\n",
    "    - theta_hourly_main (np.ndarray): Main hourly abandonment rates.\n",
    "    - agents (np.ndarray): Array of agents.\n",
    "    - old_class_no (int): Number of old classes.\n",
    "    - old_scaling_factor (float): Old scaling factor.\n",
    "    - precision (int): Precision for calculations.\n",
    "    - seed_value (int) : Random seed \n",
    "\n",
    "    Returns:\n",
    "    - tuple: High-dimensional parameters including arrivals, agents, mu, theta, abandonment rate, percentage and scaling factor.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed_value)\n",
    "\n",
    "    #Step 1: arrival rates\n",
    "    arrivals_high_dim = np.zeros((high_dim_class_no, length_time))\n",
    "    \n",
    "    indices_arrivals = np.arange(0, main_test_class_no)\n",
    "    repeated_indices_arrivals = np.random.choice(indices_arrivals, size = high_dim_class_no, replace = True)\n",
    "    \n",
    "    for i in range(high_dim_class_no):\n",
    "        arrivals_high_dim[i,:] = arrivals_main[repeated_indices_arrivals[i],:]\n",
    "        \n",
    "    total_arrivals_high_dim = np.sum(arrivals_high_dim, axis = 0)\n",
    "\n",
    "    #Step 2: arrival percentages\n",
    "    numerator = np.sum(arrivals_high_dim, axis = 1)\n",
    "    denominator = np.sum(numerator)\n",
    "    percentage_high_dim = numerator/denominator\n",
    "\n",
    "    #Step 3: service and abandonment rates\n",
    "    mu_high_dimensional = np.zeros((high_dim_class_no,))\n",
    "    theta_high_dimensional = np.zeros((high_dim_class_no,))\n",
    "\n",
    "    indices_system_parameters = np.arange(0, main_test_class_no)\n",
    "    repeated_indices_system_parameters = np.random.choice(indices_system_parameters, size = high_dim_class_no, replace = True)\n",
    "\n",
    "    for i in range(high_dim_class_no):\n",
    "        mu_high_dimensional[i] = mu_hourly_main[repeated_indices_system_parameters[i]]\n",
    "        theta_high_dimensional[i] = theta_hourly_main[repeated_indices_system_parameters[i]]\n",
    "    \n",
    "    #Step 4: main test utilization\n",
    "    denominator = np.sum(agents)\n",
    "    num_factor = np.sum(arrivals_main, axis = 1)\n",
    "    numerator = np.sum(1/mu_hourly_main * num_factor)\n",
    "    main_test_utilization = numerator/denominator\n",
    "\n",
    "    \n",
    "    #Step 5: new system scaling parameter\n",
    "    high_dim_scaling_factor = np.round(high_dim_class_no/old_class_no*old_scaling_factor, 2)\n",
    "    high_dim_utilization = 1 - ((1 - main_test_utilization)/np.sqrt(high_dim_class_no/main_test_class_no))\n",
    "    \n",
    "    #Step 6: new staffing level\n",
    "    first_term = agents/np.sum(agents)\n",
    "    second_term = 1/high_dim_utilization\n",
    "    third_term = np.sum(1/mu_high_dimensional * np.sum(arrivals_high_dim, axis = 1))\n",
    "    high_dim_agents = first_term * second_term * third_term \n",
    "    high_dim_agents = np.ceil(high_dim_agents).astype(int) # Convert to integer\n",
    "    \n",
    "    #Step 7: system parameters\n",
    "    if high_dim_class_no == 30:\n",
    "        numbers = np.arange(14, 34, 0.5)\n",
    "    elif high_dim_class_no == 50:\n",
    "        numbers = np.arange(14, 34, 0.25)\n",
    "    elif high_dim_class_no == 100:\n",
    "        numbers = np.arange(14, 34, 0.125)\n",
    "\n",
    "    holding_cost_rate_high_dim = np.random.choice(numbers, size = high_dim_class_no, replace = False)\n",
    "    abandonment_cost_rate_high_dim = holding_cost_rate_high_dim/12\n",
    "\n",
    "    common_mu = np.round(np.sum(percentage_high_dim*mu_high_dimensional),2)\n",
    "    common_theta = np.round(np.sum(percentage_high_dim*theta_high_dimensional),2)\n",
    "    common_abandonment_rate = np.round(np.sum(percentage_high_dim*abandonment_cost_rate_high_dim),2)\n",
    "\n",
    "    cost_pathwise = np.zeros((high_dim_class_no,))\n",
    "    common_abandonment_rate = np.sum(percentage_high_dim * abandonment_cost_rate_high_dim)\n",
    "\n",
    "    for i in range(high_dim_class_no):\n",
    "        cost_pathwise[i] = holding_cost_rate_high_dim[i] + common_abandonment_rate * common_theta\n",
    "    \n",
    "    file_name_hourly_mu = os.path.join(folder_path, f\"mu_hourly_{high_dim_class_no}dim.csv\")\n",
    "    file_name_hourly_theta = os.path.join(folder_path, f\"theta_hourly_{high_dim_class_no}dim.csv\")\n",
    "    file_name_holding_cost = os.path.join(folder_path, f\"hourly_holding_cost_{high_dim_class_no}dim.csv\")\n",
    "    file_name_abandonment_cost = os.path.join(folder_path,f\"abandonment_cost_{high_dim_class_no}dim.csv\")\n",
    "    file_name_total_cost = os.path.join(folder_path, f\"hourly_total_cost_{high_dim_class_no}dim.csv\")\n",
    "    file_name_percentage = os.path.join(folder_path, f\"percentages_{high_dim_class_no}dim.csv\")\n",
    "    file_name_agents = os.path.join(folder_path, f\"agents_{high_dim_class_no}dim.csv\")\n",
    "    file_name_arrivals = os.path.join(folder_path, f\"arrivals_partial_{precision}min_{high_dim_class_no}dim.csv\")\n",
    "    file_name_total_arrivals = os.path.join(folder_path, f\"total_arrivals_partial_{precision}min_{high_dim_class_no}dim.csv\")\n",
    "    \n",
    "    np.savetxt(file_name_total_cost, cost_pathwise, delimiter = \",\", fmt=\"%.2f\")\n",
    "    np.savetxt(file_name_agents, high_dim_agents, delimiter = \",\")\n",
    "    np.savetxt(file_name_holding_cost, holding_cost_rate_high_dim, delimiter = \",\", fmt=\"%.2f\")\n",
    "    np.savetxt(file_name_abandonment_cost, np.array([common_abandonment_rate]), delimiter = \",\", fmt=\"%.2f\")\n",
    "    np.savetxt(file_name_arrivals, arrivals_high_dim / (MINUTES_IN_HOUR/precision), delimiter = \",\") ##arrival rate per 5 minutes\n",
    "    np.savetxt(file_name_percentage, percentage_high_dim * 100, delimiter = \",\", fmt=\"%.2f\")\n",
    "    np.savetxt(file_name_total_arrivals, total_arrivals_high_dim / (MINUTES_IN_HOUR/precision), delimiter = \",\")\n",
    "    np.savetxt(file_name_hourly_mu, np.array([common_mu]), delimiter = \",\", fmt=\"%.2f\")\n",
    "    np.savetxt(file_name_hourly_theta, np.array([common_theta]), delimiter = \",\", fmt=\"%.2f\")\n",
    "    \n",
    "    return arrivals_high_dim, high_dim_agents, common_mu, common_theta, common_abandonment_rate, high_dim_scaling_factor, percentage_high_dim, cost_pathwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3660579",
   "metadata": {},
   "source": [
    "### 3.2 Limiting arrival rates and zeta calculation and cdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2350d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "OLD_SCALING_FACTOR = 400\n",
    "\n",
    "mu_hourly_main = SECONDS_IN_HOUR / service_times_17dim[\"Total\"]\n",
    "theta_hourly_main = SECONDS_IN_HOUR / abandonment_times_17dim[\"Total\"]\n",
    "\n",
    "#30 dimensional test problem\n",
    "high_dim_class_no = 30\n",
    "\n",
    "folder_path_30dim = os.path.join(data_folder_path,f\"problem_{high_dim_class_no}dim\")\n",
    "os.makedirs(folder_path_30dim, exist_ok=True)\n",
    "\n",
    "arrivals_high_dim_30dim, high_dim_agents_30dim, common_mu_30dim, common_theta_30dim, common_abandonment_rate_30dim, high_dim_scaling_factor_30dim, percentage_high_dim_30dim, cost_pathwise_30dim = calculate_high_dimensional_parameters(folder_path_30dim, high_dim_class_no, main_test_class_no, lambd_prelimit_hourly, mu_hourly_main, theta_hourly_main, agents, main_test_class_no, OLD_SCALING_FACTOR, precision, seed_value)\n",
    "lambd_limit_hourly_30dim, zeta_hourly_30dim = compute_limiting_hourly_rates(high_dim_class_no, arrivals_high_dim_30dim, time_focus_start, time_focus_end, precision, common_mu_30dim, percentage_high_dim_30dim, high_dim_agents_30dim, length_time, high_dim_scaling_factor_30dim)\n",
    "\n",
    "# Calculate cumulative sum of arrivals for each time interval\n",
    "cumsum_30dim = np.cumsum(arrivals_high_dim_30dim, axis = 0)\n",
    "cumsum_30dim = cumsum_30dim.T\n",
    "total_arrivals_reshaped  = np.array(cumsum_30dim[:,high_dim_class_no-1]).reshape(-1, 1)\n",
    "\n",
    "# Calculate CDF by normalizing cumulative sums with total arrivals\n",
    "cdf_30dim = cumsum_30dim / total_arrivals_reshaped\n",
    "\n",
    "# Save the CDF to a CSV file\n",
    "file_name_cdf = os.path.join(folder_path_30dim, f\"cdf_{high_dim_class_no}dim.csv\")\n",
    "np.savetxt(file_name_cdf, cdf_30dim, delimiter=\",\", fmt=\"%.3f\")\n",
    "\n",
    "file_name_lambda_limiting = os.path.join(folder_path_30dim, f\"hourly_limiting_lambda_{high_dim_class_no}dim.csv\")\n",
    "file_name_zeta = os.path.join(folder_path_30dim, f\"hourly_zeta_{high_dim_class_no}dim.csv\")\n",
    "\n",
    "np.savetxt(file_name_lambda_limiting, lambd_limit_hourly_30dim, delimiter = \",\")\n",
    "np.savetxt(file_name_zeta, zeta_hourly_30dim, delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ac3c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#50 dimensional test problem\n",
    "high_dim_class_no = 50\n",
    "\n",
    "folder_path_50dim = os.path.join(data_folder_path,f\"problem_{high_dim_class_no}dim\")\n",
    "os.makedirs(folder_path_50dim, exist_ok=True)\n",
    "\n",
    "arrivals_high_dim_50dim, high_dim_agents_50dim, common_mu_50dim, common_theta_50dim, common_abandonment_rate_50dim, high_dim_scaling_factor_50dim, percentage_high_dim_50dim, cost_pathwise_50dim = calculate_high_dimensional_parameters(folder_path_50dim, high_dim_class_no, main_test_class_no, lambd_prelimit_hourly, mu_hourly_main, theta_hourly_main, agents, main_test_class_no, OLD_SCALING_FACTOR, precision, seed_value)\n",
    "lambd_limit_hourly_50dim, zeta_hourly_50dim = compute_limiting_hourly_rates(high_dim_class_no, arrivals_high_dim_50dim, time_focus_start, time_focus_end, precision, common_mu_50dim, percentage_high_dim_50dim, high_dim_agents_50dim, length_time, high_dim_scaling_factor_50dim)\n",
    "\n",
    "# Calculate cumulative sum of arrivals for each time interval\n",
    "cumsum_50dim = np.cumsum(arrivals_high_dim_50dim, axis = 0)\n",
    "cumsum_50dim = cumsum_50dim.T\n",
    "total_arrivals_reshaped  = np.array(cumsum_50dim[:,high_dim_class_no-1]).reshape(-1, 1)\n",
    "\n",
    "# Calculate CDF by normalizing cumulative sums with total arrivals\n",
    "cdf_50dim = cumsum_50dim / total_arrivals_reshaped\n",
    "\n",
    "# Save the CDF to a CSV file\n",
    "file_name_cdf = os.path.join(folder_path_50dim, f\"cdf_{high_dim_class_no}dim.csv\")\n",
    "np.savetxt(file_name_cdf, cdf_50dim, delimiter=\",\", fmt=\"%.3f\")\n",
    "\n",
    "file_name_lambda_limiting = os.path.join(folder_path_50dim, f\"hourly_limiting_lambda_{high_dim_class_no}dim.csv\")\n",
    "file_name_zeta = os.path.join(folder_path_50dim, f\"hourly_zeta_{high_dim_class_no}dim.csv\")\n",
    "\n",
    "np.savetxt(file_name_lambda_limiting, lambd_limit_hourly_50dim, delimiter = \",\")\n",
    "np.savetxt(file_name_zeta, zeta_hourly_50dim, delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c3d25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#100 dimensional test problem\n",
    "high_dim_class_no = 100\n",
    "\n",
    "folder_path_100dim = os.path.join(data_folder_path,f\"problem_{high_dim_class_no}dim\")\n",
    "os.makedirs(folder_path_100dim, exist_ok=True)\n",
    "\n",
    "arrivals_high_dim_100dim, high_dim_agents_100dim, common_mu_100dim, common_theta_100dim, common_abandonment_rate_100dim, high_dim_scaling_factor_100dim, percentage_high_dim_100dim, cost_pathwise_100dim = calculate_high_dimensional_parameters(folder_path_100dim, high_dim_class_no, main_test_class_no, lambd_prelimit_hourly, mu_hourly_main, theta_hourly_main, agents, main_test_class_no, OLD_SCALING_FACTOR, precision, seed_value)\n",
    "lambd_limit_hourly_100dim, zeta_hourly_100dim = compute_limiting_hourly_rates(high_dim_class_no, arrivals_high_dim_100dim, time_focus_start, time_focus_end, precision, common_mu_100dim, percentage_high_dim_100dim, high_dim_agents_100dim, length_time, high_dim_scaling_factor_100dim)\n",
    "\n",
    "# we need to calculate the cdf of arrivals to use in our discrete event simulation to determine which class arrives\n",
    "\n",
    "# Calculate cumulative sum of arrivals for each time interval\n",
    "cumsum_100dim = np.cumsum(arrivals_high_dim_100dim, axis = 0)\n",
    "cumsum_100dim = cumsum_100dim.T\n",
    "total_arrivals_reshaped  = np.array(cumsum_100dim[:,high_dim_class_no-1]).reshape(-1, 1)\n",
    "\n",
    "# Calculate CDF by normalizing cumulative sums with total arrivals\n",
    "cdf_100dim = cumsum_100dim / total_arrivals_reshaped\n",
    "\n",
    "# Save the CDF to a CSV file\n",
    "file_name_cdf = os.path.join(folder_path_100dim, f\"cdf_{high_dim_class_no}dim.csv\")\n",
    "np.savetxt(file_name_cdf, cdf_100dim, delimiter=\",\", fmt=\"%.3f\")\n",
    "\n",
    "file_name_lambda_limiting = os.path.join(folder_path_100dim, f\"hourly_limiting_lambda_{high_dim_class_no}dim.csv\")\n",
    "file_name_zeta = os.path.join(folder_path_100dim, f\"hourly_zeta_{high_dim_class_no}dim.csv\")\n",
    "\n",
    "np.savetxt(file_name_lambda_limiting, lambd_limit_hourly_100dim, delimiter = \",\")\n",
    "np.savetxt(file_name_zeta, zeta_hourly_100dim, delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc477b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
