{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(73)\n",
    "np.random.seed(73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------Auxiliary Functions----------------\n",
    "\n",
    "def generate_dw_sample(dim, num_time_interval, num_sample, sqrt_delta_t):\n",
    "    \"\"\"Generate Brownian motion increments.\"\"\"\n",
    "    return np.random.randn(dim, num_time_interval, num_sample) * sqrt_delta_t\n",
    "\n",
    "def generate_u_sample(dim, policy, num_sample):\n",
    "    \"\"\"Generate reference policies.\"\"\"\n",
    "    \n",
    "    if POLICY == \"even\": \n",
    "        u_sample = np.ones((dim, num_sample)) * 1/dim\n",
    "    \n",
    "    elif POLICY == \"random\":\n",
    "        u_sample = np.random.dirichlet(np.ones(dim), num_sample).T\n",
    "        \n",
    "    elif POLICY == \"minimal\": \n",
    "        u_sample = np.zeros((dim, num_sample))\n",
    "    \n",
    "    elif POLICY in {\"best_2dim\", \"best_3dim\"}:\n",
    "        u_sample = np.zeros((dim, num_sample))\n",
    "        u_sample[1,:] = np.ones(num_sample)\n",
    "        \n",
    "    elif POLICY == \"best_3dim_var\": \n",
    "        u_sample = np.zeros((dim, num_sample))\n",
    "        u_sample[0,:] = np.ones(num_sample)\n",
    "        \n",
    "    elif POLICY == \"weighted_split_main\":\n",
    "        u_sample = np.ones((DIM, NUM_SAMPLE)) * 0.037178922\n",
    "        u_sample[0, :] = 0.159803922\n",
    "        u_sample[1, :] = 0.159803922\n",
    "        u_sample[2, :] = 0.159803922\n",
    "    \n",
    "    elif POLICY == \"weighted_split_var1\":\n",
    "        alpha = 4/6\n",
    "        base_weight = alpha * 0.025 + (1 - alpha) * 1/dim\n",
    "        special_weight = alpha * 0.14 + (1 - alpha) * 1/dim\n",
    "        u_sample = np.ones((dim, num_sample)) * base_weight\n",
    "        u_sample[0,:] = special_weight\n",
    "        u_sample[1,:] = special_weight\n",
    "        u_sample[2,:] = special_weight\n",
    "        u_sample[6,:] = special_weight\n",
    "        u_sample[14,:] = special_weight\n",
    "        \n",
    "    elif POLICY == \"weighted_split_var2\":\n",
    "        alpha = 3/6\n",
    "        base_weight = alpha * 0.025 + (1 - alpha) * 1/dim\n",
    "        special_weight = alpha * 0.14 + (1 - alpha) * 1/dim\n",
    "        u_sample = np.ones((dim, num_sample)) * base_weight\n",
    "        u_sample[0,:] = special_weight\n",
    "        u_sample[1,:] = special_weight\n",
    "        u_sample[2,:] = special_weight\n",
    "        u_sample[6,:] = special_weight\n",
    "        u_sample[14,:] = special_weight\n",
    "        \n",
    "    return u_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- Main Simulation Function --------------------------------\n",
    "\n",
    "def sample(total_time, num_time_interval, dim, num_sample, policy, lambd, zeta, mu, theta):\n",
    "\n",
    "    \"\"\"Generate sample paths for the associated reference policy.\"\"\"\n",
    "    \n",
    "    delta_t = total_time / num_time_interval\n",
    "    sqrt_delta_t = np.sqrt(delta_t)\n",
    "    num_five_min_interval = 204\n",
    "    \n",
    "    # Repeat values to match the required number of intervals\n",
    "    lambd = np.repeat(lambd, num_time_interval // num_five_min_interval, axis=1)\n",
    "    zeta = np.repeat(zeta, num_time_interval // num_five_min_interval, axis=1)\n",
    "    \n",
    "    sigma = np.sqrt(2 * lambd)   # Diffusion coefficient\n",
    "    \n",
    "    # Generate noise and control samples\n",
    "    dw_sample = generate_dw_sample(dim, num_time_interval, num_sample, sqrt_delta_t)\n",
    "    u_sample = generate_u_sample(dim, policy, num_sample)\n",
    "    \n",
    "    # Initialize sample paths\n",
    "    x_sample = np.zeros((dim, num_time_interval + 1, num_sample)) \n",
    "    x_sample[:,0,:] = np.random.uniform(-10, 10, (dim, num_sample)) \n",
    "\n",
    "    for i in range(num_time_interval):\n",
    "        \n",
    "        sum_x = np.sum(x_sample[:, i, :], axis=0, keepdims=True)  # Shape (1, num_sample)\n",
    "        mx = np.maximum(sum_x, 0.0)  # Shape (1, num_sample)\n",
    "        \n",
    "        zeta_i = zeta[:, i].reshape(-1, 1)\n",
    "        sigma_i = sigma[:, i].reshape(-1, 1)\n",
    "        \n",
    "        x_sample[:, i + 1, :] = x_sample[:, i, :] + (zeta_i - mu * x_sample[:, i, :]) * delta_t \\\n",
    "                                + sigma_i * dw_sample[:, i, :] \\\n",
    "                                + (mx * ((mu - theta) * u_sample)) * delta_t\n",
    "            \n",
    "    return x_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- Fitting Functions --------------------------------\n",
    "def fit_normal(x, dim, num_time_interval, policy, output_folder=\"output\"):\n",
    "    \"\"\"Fit normal distributions to sample paths and save mean/std per interval.\"\"\"\n",
    "\n",
    "    #foldername = \"/Users/ebrukasikaralar/100dim_main/records_\" + policy + \"_\" + str(num_time_interval) + \"_uniform_initialization\"\n",
    "    folder_path = os.path.join(output_folder, f\"{policy}_{num_time_interval}_uniform_initialization\")\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    means = np.zeros((dim, num_time_interval + 1))\n",
    "    stds = np.zeros((dim, num_time_interval + 1))\n",
    "        \n",
    "    for t in range(num_time_interval + 1):\n",
    "        \n",
    "        mean_std_file = os.path.join(folder_path, f\"mean_std_interval{t}.csv\")\n",
    "\n",
    "        save_array = np.zeros((dim, 2))\n",
    "            \n",
    "        for cls in range(dim):\n",
    "                        \n",
    "            mean, std = norm.fit(x[cls, t])\n",
    "            \n",
    "            means[cls,t] = mean\n",
    "            stds[cls,t] = std\n",
    "                        \n",
    "            save_array[cls,0] = mean\n",
    "            save_array[cls,1] = std\n",
    "            \n",
    "            np.savetxt(mean_std_file, save_array, delimiter = \",\", comments = \"\")\n",
    "        \n",
    "    return means, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saver(dim, num_time_interval, policy, output_folder=\"output\"):\n",
    "    \"\"\"Load and save aggregated mean and standard deviation data.\"\"\"\n",
    "    \n",
    "    folder_path = os.path.join(output_folder, f\"{policy}_{num_time_interval}_uniform_initialization\")\n",
    "    file_name = \"mean_std_interval\"\n",
    "    \n",
    "    means = np.zeros((dim, num_time_interval + 1))\n",
    "    stds = np.zeros((dim, num_time_interval + 1))\n",
    "    \n",
    "    for i in range(num_time_interval + 1):\n",
    "\n",
    "        file = os.path.join(folder_path, f\"{file_name}{i}.csv\")\n",
    "        data = pd.read_csv(file, header = None)\n",
    "\n",
    "        mean = data[0]\n",
    "        std = data[1]\n",
    "\n",
    "        means[:, i] = mean\n",
    "        stds[:, i] = std\n",
    "\n",
    "    np.savetxt(os.path.join(folder_path, f\"{policy}_{num_time_interval}_means.csv\"), means, delimiter=\",\")\n",
    "    np.savetxt(os.path.join(folder_path, f\"{policy}_{num_time_interval}_stds.csv\"), stds, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- Sample simulation parameters --------------------------------\n",
    "TOTAL_TIME = 17\n",
    "NUM_TIME_INTERVAL = 1020\n",
    "DIM = 17\n",
    "NUM_SAMPLE = 5000\n",
    "POLICY = \"weighted_split_main\"\n",
    "\n",
    "# Load Data\n",
    "MU = pd.read_csv(\"/Users/ebrukasikaralar/sample_path_generation/config_main/mu_hourly_17dim.csv\", header=None).values.reshape((DIM, 1))\n",
    "THETA = pd.read_csv(\"/Users/ebrukasikaralar/sample_path_generation/config_main/theta_hourly_17dim.csv\", header=None).values.reshape((DIM, 1))\n",
    "LAMBD = pd.read_csv(\"/Users/ebrukasikaralar/sample_path_generation/config_main/lambd_matrix_hourly_17dim.csv\", header=None).values\n",
    "ZETA = pd.read_csv(\"/Users/ebrukasikaralar/sample_path_generation/config_main/zeta_matrix_hourly_17dim.csv\", header=None).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- Run simulation --------------------------------\n",
    "x_sample = sample(TOTAL_TIME, NUM_TIME_INTERVAL, DIM, NUM_SAMPLE, POLICY, LAMBD, ZETA, MU, THETA)\n",
    "\n",
    "# Fit normal distributions and save results\n",
    "fit_normal(x_sample, DIM, NUM_TIME_INTERVAL, POLICY)\n",
    "\n",
    "# Save aggregated results\n",
    "saver(DIM, NUM_TIME_INTERVAL, POLICY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
